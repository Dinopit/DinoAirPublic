name: Performance Regression Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests nightly at 1 AM UTC
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test duration in minutes'
        required: false
        default: '5'
      load_level:
        description: 'Load testing level'
        required: false
        default: 'light'
        type: choice
        options:
          - light
          - moderate
          - heavy

permissions:
  contents: read
  actions: write

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'
  TEST_DURATION: ${{ github.event.inputs.test_duration || '5' }}
  LOAD_LEVEL: ${{ github.event.inputs.load_level || 'light' }}

jobs:
  setup-performance-environment:
    name: Setup Performance Testing Environment
    runs-on: ubuntu-latest
    outputs:
      baseline-exists: ${{ steps.check-baseline.outputs.exists }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for performance baseline
        id: check-baseline
        run: |
          if [ -f .github/performance-baselines/baseline.json ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Create performance directory
        run: |
          mkdir -p .github/performance-baselines
          mkdir -p performance-results

  cli-performance-tests:
    name: CLI Performance Testing
    runs-on: ubuntu-latest
    needs: setup-performance-environment
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install performance testing tools
        run: |
          # Install hyperfine for CLI benchmarking
          curl -L https://github.com/sharkdp/hyperfine/releases/latest/download/hyperfine_1.18.0_amd64.deb -o hyperfine.deb
          sudo dpkg -i hyperfine.deb || sudo apt-get install -f
          
          # Install Python dependencies
          pip install -r requirements.txt || true
          pip install memory-profiler psutil

      - name: Benchmark CLI startup time
        run: |
          echo "# CLI Performance Test Results" > cli-performance.md
          echo "" >> cli-performance.md
          echo "## Startup Performance" >> cli-performance.md
          
          hyperfine --warmup 3 --runs 10 \
            --export-json cli-startup-results.json \
            --export-markdown cli-startup.md \
            'python start.py --help'
          
          cat cli-startup.md >> cli-performance.md

      - name: Benchmark configuration loading
        run: |
          echo "" >> cli-performance.md
          echo "## Configuration Loading Performance" >> cli-performance.md
          
          hyperfine --warmup 2 --runs 5 \
            --export-json config-loading-results.json \
            --export-markdown config-loading.md \
            'python -c "from lib.config import load_config; load_config()"'
          
          cat config-loading.md >> cli-performance.md

      - name: Memory usage profiling
        run: |
          echo "" >> cli-performance.md
          echo "## Memory Usage Profile" >> cli-performance.md
          
          # Profile memory usage during startup
          python -m memory_profiler start.py --help > memory-profile.txt 2>&1 || true
          
          echo "\`\`\`" >> cli-performance.md
          cat memory-profile.txt >> cli-performance.md
          echo "\`\`\`" >> cli-performance.md

      - name: Compare with baseline
        if: needs.setup-performance-environment.outputs.baseline-exists == 'true'
        run: |
          echo "" >> cli-performance.md
          echo "## Baseline Comparison" >> cli-performance.md
          
          if [ -f .github/performance-baselines/cli-baseline.json ]; then
            python -c "
          import json
          import sys
          
          # Load current results
          with open('cli-startup-results.json') as f:
              current = json.load(f)
          
          # Load baseline
          with open('.github/performance-baselines/cli-baseline.json') as f:
              baseline = json.load(f)
          
          current_mean = current['results'][0]['mean']
          baseline_mean = baseline['results'][0]['mean']
          
          regression = ((current_mean - baseline_mean) / baseline_mean) * 100
          
          print(f'Current startup time: {current_mean:.3f}s')
          print(f'Baseline startup time: {baseline_mean:.3f}s')
          print(f'Performance change: {regression:+.1f}%')
          
          if regression > 10:
              print('‚ö†Ô∏è Performance regression detected!')
              sys.exit(1)
          elif regression < -5:
              print('üöÄ Performance improvement detected!')
          else:
              print('‚úÖ Performance within acceptable range')
          " >> cli-performance.md
          else
            echo "No CLI baseline found, creating new baseline" >> cli-performance.md
            cp cli-startup-results.json .github/performance-baselines/cli-baseline.json
          fi

      - name: Upload CLI performance results
        uses: actions/upload-artifact@v4
        with:
          name: cli-performance-results
          path: |
            cli-performance.md
            cli-startup-results.json
            config-loading-results.json
            memory-profile.txt
          retention-days: 30

  web-performance-tests:
    name: Web Application Performance Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          cd web-gui-node && npm install || true
          cd ../web-gui && npm install || true

      - name: Install performance testing tools
        run: |
          npm install -g lighthouse artillery k6

      - name: Start web application
        run: |
          cd web-gui-node
          npm start &
          echo $! > ../server.pid
          sleep 15
        continue-on-error: true

      - name: Run Lighthouse performance audit
        run: |
          echo "# Web Performance Test Results" > web-performance.md
          echo "" >> web-performance.md
          echo "## Lighthouse Audit Results" >> web-performance.md
          
          lighthouse http://localhost:3000 \
            --output json --output html \
            --output-path lighthouse-results \
            --only-categories=performance \
            --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage"
          
          # Extract performance score
          PERF_SCORE=$(cat lighthouse-results.report.json | jq '.categories.performance.score * 100')
          echo "Performance Score: $PERF_SCORE/100" >> web-performance.md

      - name: Run load testing with Artillery
        run: |
          echo "" >> web-performance.md
          echo "## Load Testing Results (${{ env.LOAD_LEVEL }} load)" >> web-performance.md
          
          # Create Artillery config based on load level
          case "${{ env.LOAD_LEVEL }}" in
            "light")
              VU_COUNT=5
              DURATION=60
              ;;
            "moderate")
              VU_COUNT=20
              DURATION=120
              ;;
            "heavy")
              VU_COUNT=50
              DURATION=300
              ;;
          esac
          
          cat > artillery-config.yml << EOF
          config:
            target: 'http://localhost:3000'
            phases:
              - duration: ${DURATION}
                arrivalRate: ${VU_COUNT}
          scenarios:
            - name: "Homepage load test"
              requests:
                - get:
                    url: "/"
          EOF
          
          artillery run artillery-config.yml --output artillery-results.json
          
          # Generate report
          artillery report artillery-results.json --output artillery-report.html

      - name: Run K6 performance tests
        run: |
          echo "" >> web-performance.md
          echo "## K6 Performance Testing" >> web-performance.md
          
          cat > k6-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export let options = {
            stages: [
              { duration: '30s', target: 5 },
              { duration: '60s', target: 10 },
              { duration: '30s', target: 0 },
            ],
          };
          
          export default function() {
            let response = http.get('http://localhost:3000');
            check(response, {
              'status is 200': (r) => r.status === 200,
              'response time < 500ms': (r) => r.timings.duration < 500,
            });
            sleep(1);
          }
          EOF
          
          k6 run --out json=k6-results.json k6-test.js || true

      - name: Analyze performance metrics
        run: |
          echo "" >> web-performance.md
          echo "## Performance Analysis" >> web-performance.md
          
          python -c "
          import json
          import statistics
          
          # Analyze Lighthouse results
          try:
              with open('lighthouse-results.report.json') as f:
                  lighthouse = json.load(f)
              
              perf_score = lighthouse['categories']['performance']['score'] * 100
              fcp = lighthouse['audits']['first-contentful-paint']['numericValue'] / 1000
              lcp = lighthouse['audits']['largest-contentful-paint']['numericValue'] / 1000
              
              print(f'Performance Score: {perf_score:.1f}/100')
              print(f'First Contentful Paint: {fcp:.2f}s')
              print(f'Largest Contentful Paint: {lcp:.2f}s')
          except Exception as e:
              print(f'Could not analyze Lighthouse results: {e}')
          
          # Analyze K6 results
          try:
              with open('k6-results.json') as f:
                  k6_data = [json.loads(line) for line in f if line.strip()]
              
              http_reqs = [d for d in k6_data if d.get('type') == 'Point' and d.get('metric') == 'http_req_duration']
              if http_reqs:
                  durations = [d['data']['value'] for d in http_reqs]
                  avg_duration = statistics.mean(durations)
                  p95_duration = statistics.quantiles(durations, n=20)[18]  # 95th percentile
                  
                  print(f'Average Response Time: {avg_duration:.2f}ms')
                  print(f'95th Percentile: {p95_duration:.2f}ms')
          except Exception as e:
              print(f'Could not analyze K6 results: {e}')
          " >> web-performance.md

      - name: Stop web application
        if: always()
        run: |
          if [ -f server.pid ]; then
            kill $(cat server.pid) || true
            rm server.pid
          fi
          pkill -f "npm start" || true

      - name: Upload web performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: web-performance-results
          path: |
            web-performance.md
            lighthouse-results.report.json
            lighthouse-results.report.html
            artillery-results.json
            artillery-report.html
            k6-results.json
          retention-days: 30

  api-performance-tests:
    name: API Performance Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt || true
          pip install locust requests
          cd web-gui-node && npm install || true

      - name: Start API server
        run: |
          cd web-gui-node
          npm start &
          echo $! > ../api-server.pid
          sleep 15
        continue-on-error: true

      - name: Run API load tests with Locust
        run: |
          echo "# API Performance Test Results" > api-performance.md
          echo "" >> api-performance.md
          echo "## API Load Testing" >> api-performance.md
          
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          
          class APIUser(HttpUser):
              wait_time = between(1, 3)
              
              @task(3)
              def get_homepage(self):
                  self.client.get("/")
              
              @task(1)
              def get_api_status(self):
                  self.client.get("/api/status", catch_response=True)
          EOF
          
          # Run Locust in headless mode
          locust -f locustfile.py --headless \
            --users 10 --spawn-rate 2 \
            --run-time ${TEST_DURATION}m \
            --host http://localhost:3000 \
            --html api-load-report.html \
            --csv api-load-results || true

      - name: Benchmark API endpoints
        run: |
          echo "" >> api-performance.md
          echo "## Individual Endpoint Benchmarks" >> api-performance.md
          
          # Install hyperfine if not available
          which hyperfine || (
            curl -L https://github.com/sharkdp/hyperfine/releases/latest/download/hyperfine_1.18.0_amd64.deb -o hyperfine.deb
            sudo dpkg -i hyperfine.deb || sudo apt-get install -f
          )
          
          # Benchmark key endpoints
          echo "### Homepage Endpoint" >> api-performance.md
          hyperfine --warmup 3 --runs 20 \
            --export-markdown homepage-bench.md \
            'curl -s http://localhost:3000/' || true
          cat homepage-bench.md >> api-performance.md || true
          
          echo "### API Status Endpoint" >> api-performance.md
          hyperfine --warmup 3 --runs 20 \
            --export-markdown api-status-bench.md \
            'curl -s http://localhost:3000/api/status' || true
          cat api-status-bench.md >> api-performance.md || true

      - name: Stop API server
        if: always()
        run: |
          if [ -f api-server.pid ]; then
            kill $(cat api-server.pid) || true
            rm api-server.pid
          fi
          pkill -f "npm start" || true

      - name: Upload API performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: api-performance-results
          path: |
            api-performance.md
            api-load-report.html
            api-load-results_*
            locustfile.py
          retention-days: 30

  performance-regression-analysis:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [cli-performance-tests, web-performance-tests, api-performance-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all performance results
        uses: actions/download-artifact@v4
        with:
          path: performance-results

      - name: Generate performance summary
        run: |
          echo "# Performance Regression Testing Summary" > performance-summary.md
          echo "" >> performance-summary.md
          echo "## Test Configuration:" >> performance-summary.md
          echo "- Test Duration: ${{ env.TEST_DURATION }} minutes" >> performance-summary.md
          echo "- Load Level: ${{ env.LOAD_LEVEL }}" >> performance-summary.md
          echo "- Date: $(date)" >> performance-summary.md
          echo "" >> performance-summary.md
          echo "## Test Results:" >> performance-summary.md
          echo "- CLI Performance: ${{ needs.cli-performance-tests.result }}" >> performance-summary.md
          echo "- Web Performance: ${{ needs.web-performance-tests.result }}" >> performance-summary.md
          echo "- API Performance: ${{ needs.api-performance-tests.result }}" >> performance-summary.md
          echo "" >> performance-summary.md
          echo "## Performance Reports:" >> performance-summary.md
          find performance-results -name "*.md" -o -name "*.html" -o -name "*.json" | head -20 >> performance-summary.md
          echo "" >> performance-summary.md
          echo "## Recommendations:" >> performance-summary.md
          echo "- Monitor response times for critical endpoints" >> performance-summary.md
          echo "- Set up performance budgets for key metrics" >> performance-summary.md
          echo "- Implement caching strategies for slow endpoints" >> performance-summary.md
          echo "- Consider CDN for static assets" >> performance-summary.md

      - name: Check for performance regressions
        run: |
          echo "" >> performance-summary.md
          echo "## Regression Analysis:" >> performance-summary.md
          
          # Simple regression check (in a real scenario, you'd compare with historical data)
          if find performance-results -name "*performance.md" -exec grep -l "regression detected" {} \; | head -1; then
            echo "‚ö†Ô∏è Performance regressions detected!" >> performance-summary.md
            echo "regression_detected=true" >> $GITHUB_ENV
          else
            echo "‚úÖ No significant performance regressions detected" >> performance-summary.md
            echo "regression_detected=false" >> $GITHUB_ENV
          fi

      - name: Update performance baselines
        if: github.ref == 'refs/heads/main' && env.regression_detected == 'false'
        run: |
          mkdir -p .github/performance-baselines
          
          # Update baselines with current results
          find performance-results -name "*-results.json" -exec cp {} .github/performance-baselines/ \;
          
          echo "Updated performance baselines" >> performance-summary.md

      - name: Upload performance summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-regression-summary
          path: performance-summary.md
          retention-days: 30

      - name: Fail if regression detected
        if: env.regression_detected == 'true'
        run: |
          echo "Performance regression detected! Check the performance reports for details."
          exit 1